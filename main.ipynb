{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":604620,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":453419,"modelId":469716}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DATA_DIR = \"./data\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:44:15.011405Z","iopub.execute_input":"2025-10-11T15:44:15.011571Z","iopub.status.idle":"2025-10-11T15:44:15.017956Z","shell.execute_reply.started":"2025-10-11T15:44:15.011554Z","shell.execute_reply":"2025-10-11T15:44:15.017300Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n!mkdir -p $DATA_DIR\n!test ! -d $DATA_DIR/train \\\n    && wget -O $DATA_DIR/train.tar https://people.eecs.berkeley.edu/~hendrycks/streethazards_train.tar \\\n    && tar -xf $DATA_DIR/train.tar -C $DATA_DIR \\\n    && rm -r $DATA_DIR/train.tar \\\n    && mv $DATA_DIR/train $DATA_DIR/streethazards_train\n!test ! -d $DATA_DIR/test \\\n    && wget -O $DATA_DIR/test.tar https://people.eecs.berkeley.edu/~hendrycks/streethazards_test.tar \\\n    && tar -xf $DATA_DIR/test.tar -C $DATA_DIR \\\n    && rm -r $DATA_DIR/test.tar\\\n    && mv $DATA_DIR/test $DATA_DIR/streethazards_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:44:17.651159Z","iopub.execute_input":"2025-10-11T15:44:17.651443Z","iopub.status.idle":"2025-10-11T15:58:07.963082Z","shell.execute_reply.started":"2025-10-11T15:44:17.651411Z","shell.execute_reply":"2025-10-11T15:58:07.962311Z"}},"outputs":[{"name":"stdout","text":"--2025-10-11 15:44:17--  https://people.eecs.berkeley.edu/~hendrycks/streethazards_train.tar\nResolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\nConnecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9386226176 (8.7G) [application/x-tar]\nSaving to: ‘./data/train.tar’\n\n./data/train.tar    100%[===================>]   8.74G  12.0MB/s    in 11m 4s  \n\n2025-10-11 15:55:22 (13.5 MB/s) - ‘./data/train.tar’ saved [9386226176/9386226176]\n\n--2025-10-11 15:55:42--  https://people.eecs.berkeley.edu/~hendrycks/streethazards_test.tar\nResolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\nConnecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2150484992 (2.0G) [application/x-tar]\nSaving to: ‘./data/test.tar’\n\n./data/test.tar     100%[===================>]   2.00G  14.7MB/s    in 2m 22s  \n\n2025-10-11 15:58:05 (14.5 MB/s) - ‘./data/test.tar’ saved [2150484992/2150484992]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:58:07.964603Z","iopub.execute_input":"2025-10-11T15:58:07.964892Z","iopub.status.idle":"2025-10-11T15:59:32.085290Z","shell.execute_reply.started":"2025-10-11T15:58:07.964867Z","shell.execute_reply":"2025-10-11T15:59:32.084603Z"}},"outputs":[{"name":"stdout","text":"Collecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.33.1)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation-models-pytorch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation-models-pytorch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom enum import IntEnum\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport segmentation_models_pytorch as smp\nfrom typing import Optional, Callable, Union, Tuple, Dict, List\nimport json\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torchvision import transforms\nfrom torchvision.transforms import v2\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T16:46:49.460487Z","iopub.execute_input":"2025-10-11T16:46:49.461170Z","iopub.status.idle":"2025-10-11T16:46:49.467950Z","shell.execute_reply.started":"2025-10-11T16:46:49.461123Z","shell.execute_reply":"2025-10-11T16:46:49.467320Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"\"\"\"\nSource: https://github.com/hendrycks/anomaly-seg/issues/15#issuecomment-890300278\n\"\"\"\nCOLORS = np.array([\n    [ 70,  70,  70],  # building     =   0,\n    [190, 153, 153],  # fence        =   1, \n    [250, 170, 160],  # other        =   2,\n    [220,  20,  60],  # pedestrian   =   3, \n    [153, 153, 153],  # pole         =   4,\n    [157, 234,  50],  # road line    =   5, \n    [128,  64, 128],  # road         =   6,\n    [244,  35, 232],  # sidewalk     =   7,\n    [107, 142,  35],  # vegetation   =   8, \n    [  0,   0, 142],  # car          =   9,\n    [102, 102, 156],  # wall         =  10, \n    [220, 220,   0],  # traffic sign =  11,\n    [ 60, 250, 240],  # anomaly      =  12,\n]) \n\n\ndef visualize_annotation(annotation_img: np.ndarray | torch.Tensor, ax=None, title= None) -> None:\n    \"\"\"\n    Visualize a segmentation annotation using a predefined color palette.\n\n    Args:\n        annotation_img (np.ndarray | torch.Tensor): 2D array with class indices.\n        ax (matplotlib.axes.Axes, optional): Axis to plot on. If None, uses current axis.\n    \"\"\"\n    if ax is None: ax = plt.gca()\n    annotation_img = np.asarray(annotation_img)\n    img_new = np.zeros((*annotation_img.shape, 3))\n\n    for index, color in enumerate(COLORS):\n        img_new[annotation_img == index] = color\n\n    ax.imshow(img_new / 255.0)\n    if title:\n        ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ndef visualize_scene(image: np.ndarray | torch.Tensor, ax=None, title= None) -> None:\n    \"\"\"\n    Visualize a raw RGB scene image.\n\n    Args:\n        image (np.ndarray | torch.Tensor): Image tensor or array in [C, H, W] or [H, W, C] format.\n        ax (matplotlib.axes.Axes, optional): Axis to plot on. If None, uses current axis.\n    \"\"\"\n    if ax is None: ax = plt.gca()\n    image = np.asarray(image)\n    ax.imshow(np.moveaxis(image, 0, -1))\n    if title:\n        ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n\nclass StreetHazardsClasses(IntEnum):\n    BUILDING        = 0\n    FENCE           = 1\n    OTHER           = 2\n    PEDESTRIAN      = 3\n    POLE            = 4\n    ROAD_LINE       = 5\n    ROAD            = 6\n    SIDEWALK        = 7\n    VEGETATION      = 8\n    CAR             = 9\n    WALL            = 10\n    TRAFFIC_SIGN    = 11\n    ANOMALY         = 12\n    \n#path to streethazards dataset\ntrain_odgt_file = f\"{DATA_DIR}/streethazards_train/train.odgt\"\nval_odgt_file = f\"{DATA_DIR}/streethazards_train/validation.odgt\"\ntest_odgt_file = f\"{DATA_DIR}/streethazards_test/test.odgt\"\n\nCOMPUTE_MEAN_STD = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:59:43.262636Z","iopub.execute_input":"2025-10-11T15:59:43.262859Z","iopub.status.idle":"2025-10-11T15:59:43.271194Z","shell.execute_reply.started":"2025-10-11T15:59:43.262841Z","shell.execute_reply":"2025-10-11T15:59:43.270549Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class StreetHazardsDataset(Dataset):\n    \"\"\"\n    A custom PyTorch Dataset for the StreetHazards inliner dataset.\n\n    This dataset reads image and segmentation label paths from a `.odgt` file,\n    applies optional resizing and spatial transformations, and returns\n    dictionary-style samples with normalized image tensors and label tensors.\n\n    Args:\n        odgt_file (str): Path to the `.odgt` file containing image and label metadata.\n        image_resize (Tuple[int, int], optional): Target size to resize images and labels. \n        spatial_transforms (Callable, optional): Optional transformation function applied to both images and labels.\n        mean_std (Tuple[List[float], List[float]], optional): Mean and standard deviation for image normalization.\n        \n    \"\"\"\n    def __init__(\n        self,\n        odgt_file: str,\n        image_resize: Tuple[int, int] = (512, 896),\n        spatial_transforms: Optional[Callable] = None,\n        mean_std: Tuple[List[float], List[float]] = None\n    ):\n\n        self.spatial_transforms = spatial_transforms\n        self.mean_std = mean_std\n        self.image_resize = image_resize\n\n        with open(odgt_file, \"r\") as f:\n            odgt_data = json.load(f)\n        \n\n        self.paths = [\n            {\n                \"image\": os.path.join(Path(odgt_file).parent, data[\"fpath_img\"]),\n                \"labels\": os.path.join(Path(odgt_file).parent, data[\"fpath_segm\"]),\n            }\n            for data in odgt_data \n        ]\n    \n    def __len__(self) -> int:\n        return len(self.paths)\n\n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n\n        image = Image.open(self.paths[idx][\"image\"]).convert(\"RGB\")\n        labels = Image.open(self.paths[idx][\"labels\"])\n\n        if self.image_resize:\n            image = transforms.Resize(self.image_resize, transforms.InterpolationMode.BILINEAR)(image)\n            labels = transforms.Resize(self.image_resize, transforms.InterpolationMode.NEAREST)(labels)\n            \n        if self.spatial_transforms:\n            image, labels  = self.spatial_transforms(image, labels)         \n\n        #to_tensor\n        image = transforms.ToTensor()(image)\n        labels = torch.as_tensor(transforms.functional.pil_to_tensor(labels), dtype=torch.int64) - 1\n        \n        labels = labels.squeeze(0)\n        \n        if self.mean_std:\n            image = transforms.Normalize(mean = self.mean_std[0], std = self.mean_std[1])(image)\n\n        return {'image' : image, 'labels' : labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:59:43.271926Z","iopub.execute_input":"2025-10-11T15:59:43.272156Z","iopub.status.idle":"2025-10-11T15:59:43.291291Z","shell.execute_reply.started":"2025-10-11T15:59:43.272133Z","shell.execute_reply":"2025-10-11T15:59:43.290681Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_one_hot_prototypes_torch(num_known_classes: int, t_value: float = 3.0, device: str = 'cpu') -> torch.Tensor:\n    \"\"\"\n    Generates one-hot prototypes as a PyTorch tensor for a given number of known classes.\n    Each prototype is a vector where only the element corresponding\n    to its class index has the 't_value', and all other elements are 0.\n\n    Args:\n        num_known_classes (int): The total number of known (in-distribution) classes.\n                                 This also determines the dimensionality of each prototype vector.\n        t_value (float): The non-zero value at the class's specific index in the prototype.\n                         As specified in the paper, this is often 3.0.\n        device (str): The device on which to create the tensor ('cpu' or 'cuda').\n\n    Returns:\n        torch.Tensor: A 2D PyTorch tensor where each row is a prototype vector.\n                      The shape will be (num_known_classes, num_known_classes).\n    \"\"\"\n    if not isinstance(num_known_classes, int) or num_known_classes <= 0:\n        raise ValueError(\"num_known_classes must be a positive integer.\")\n    if not isinstance(t_value, (int, float)):\n        raise ValueError(\"t_value must be a numeric type.\")\n    if device not in ['cpu', 'cuda']:\n        raise ValueError(\"device must be 'cpu' or 'cuda'.\")\n\n    # Create a tensor of zeros\n    prototypes = torch.zeros((num_known_classes, num_known_classes), dtype=torch.float32, device=device)\n\n    # Fill the diagonal with t_value to create one-hot prototypes\n    for i in range(num_known_classes):\n        prototypes[i, i] = t_value\n        \n    # An even more concise way using torch.eye (Identity matrix)\n    # prototypes = torch.eye(num_known_classes, dtype=torch.float32, device=device) * t_value\n\n    return prototypes\n\nPrototype  = create_one_hot_prototypes_torch(13)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:59:43.291971Z","iopub.execute_input":"2025-10-11T15:59:43.292203Z","iopub.status.idle":"2025-10-11T15:59:43.317599Z","shell.execute_reply.started":"2025-10-11T15:59:43.292179Z","shell.execute_reply":"2025-10-11T15:59:43.316982Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\nclass DMLNetFeatureExtractor(torch.nn.Module):\n    def __init__(self, encoder_name, encoder_weights, num_feature_channels, activation):\n        super().__init__()\n        \n\n        self.model = smp.DeepLabV3Plus(\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            classes=num_feature_channels, # This sets the output channels of the segmentation_head if kept\n            activation=activation # Usually 'None' for the main head, but for features it might not matter directly\n        )\n        \n\n        # Option 2 is safer and more robust.\n        # First, we disable the original segmentation head as you did.\n        self.original_segmentation_head = self.model.segmentation_head # Store it if needed\n        self.model.segmentation_head = torch.nn.Identity() # Remove the final head\n\n        # --- CORRECTION START ---\n        # To get the decoder's actual output channels, we need a dummy forward pass\n        # through just the encoder and decoder.\n        \n        # Temporarily detach the module to make a dummy pass if needed,\n        # but in __init__, we can usually just do a conceptual forward.\n        # However, to be absolutely safe and get the runtime channel count:\n        \n        # Create a dummy input to trace the decoder output channels\n        # Assuming typical RGB input (3 channels) and arbitrary spatial dimensions\n        dummy_input = torch.randn(2, 3, 256, 256) \n        \n        # Pass through encoder\n        encoder_features_dummy = self.model.encoder(dummy_input)\n        \n        # Pass through decoder to get its output channels\n        decoder_output_dummy = self.model.decoder(encoder_features_dummy)\n        \n        # Extract the channel dimension from the dummy output\n        decoder_actual_out_channels = decoder_output_dummy.shape[1]\n        # --- CORRECTION END ---\n\n        # Add a 1x1 convolution to project the decoder's output to the desired num_feature_channels.\n        self.feature_projection = torch.nn.Conv2d(\n            in_channels=decoder_actual_out_channels,\n            out_channels=num_feature_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0\n        )\n        \n    def forward(self, x):\n        input_spatial_size = x.shape[2:]\n        # The encoder outputs a list of feature maps at different resolutions\n        encoder_features = self.model.encoder(x)\n        \n        # The decoder takes these features and produces a high-resolution feature map.\n        # This output will typically have the same spatial dimensions as the input 'x'\n        # (due to DeepLabV3+ decoder's upsampling) but with its default channel count.\n        decoder_output = self.model.decoder(encoder_features)\n        \n        # Project the decoder's output to the desired number of feature channels\n        projected_features = self.feature_projection(decoder_output)\n\n        final_features = torch.nn.functional.interpolate(\n            projected_features, \n            size=input_spatial_size, \n            mode='bilinear', \n            align_corners=False # Set to True for pixel alignment if needed, but False is common\n        )\n        \n        # These `final_features` are your f(X; θf)i,j with num_feature_channels.\n        return final_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:59:43.318485Z","iopub.execute_input":"2025-10-11T15:59:43.318978Z","iopub.status.idle":"2025-10-11T15:59:43.328856Z","shell.execute_reply.started":"2025-10-11T15:59:43.318949Z","shell.execute_reply":"2025-10-11T15:59:43.328307Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class DiscriminativeCrossEntropyLoss(nn.Module):\n    def __init__(self, prototypes: torch.Tensor,lambda_weight):\n        super().__init__()\n       \n        self.prototypes = prototypes.to(device)\n        self.lambda_weight = lambda_weight\n\n    def forward(self, pixel_features: torch.Tensor, target_labels: torch.Tensor):\n        # pixel_features: (B, D, H, W)\n        # target_labels: (B, H, W)\n\n        # Reshape pixel_features for easier broadcasting with prototypes\n        # (B, D, H, W) -> (B, H, W, D)\n        pixel_features_reshaped = pixel_features.permute(0, 2, 3, 1) # (B, H, W, D)\n\n        # Expand target_labels to match the feature dimension for indexing prototypes\n        # (B, H, W) -> (B, H, W, D)\n        prototypes_target = self.prototypes[target_labels] # This should now be (B, H, W, D)\n\n        # --- Numerator (Attractive Force) ---\n        # ||f(X; θf)i,j – mY i,j ||²\n        # (B, H, W, D) - (B, H, W, D) -> (B, H, W, D)\n        difference_numerator = pixel_features_reshaped - prototypes_target\n        # (B, H, W, D) -> (B, H, W)\n        squared_diff_numerator = torch.sum(difference_numerator.pow(2), dim=-1)\n        exp_squared_diff_numerator = torch.exp(-squared_diff_numerator)\n\n        # --- Denominator (Repulsive Force) ---\n        # Σk=1 to N ( exp(-||f(X; θf)i,j – mk ||²) )\n        # To compute this, we need to calculate the squared difference for ALL prototypes\n        # and then sum their exponentials.\n\n        # Expand pixel_features to compare with all prototypes: (B, H, W, 1, D)\n        pixel_features_expanded = pixel_features_reshaped.unsqueeze(-2) # (B, H, W, 1, D)\n\n        # Expand prototypes to compare with all pixels: (1, 1, 1, N, D)\n        # self.prototypes has shape (N, D)\n        prototypes_expanded = self.prototypes.unsqueeze(0).unsqueeze(0).unsqueeze(0) # (1, 1, 1, N, D)\n\n        # Calculate difference between each pixel feature and ALL prototypes\n        # (B, H, W, 1, D) - (1, 1, 1, N, D) -> (B, H, W, N, D)\n        all_prototypes_differences = pixel_features_expanded - prototypes_expanded\n\n        # Square and sum across the feature dimension (D)\n        # (B, H, W, N, D) -> (B, H, W, N)\n        squared_diff_all_prototypes = torch.sum(all_prototypes_differences.pow(2), dim=-1)\n\n        # Exponentiate\n        # (B, H, W, N)\n        exp_squared_diff_all_prototypes = torch.exp(-squared_diff_all_prototypes)\n\n        # Sum across the prototype dimension (N) to get the denominator\n        # (B, H, W, N) -> (B, H, W)\n        denominator = torch.sum(exp_squared_diff_all_prototypes, dim=-1)\n\n        # --- Calculate Pt(Xi,j) (Equation 2) ---\n        # (B, H, W) / (B, H, W) -> (B, H, W)\n        pt_values = exp_squared_diff_numerator / (denominator + 1e-8) # Add small epsilon for stability\n\n        # --- Calculate LDCE (Equation 3) ---\n        # LDCE = -log(pt_values) for target classes\n        # This requires masking based on target_labels or using the pt_values directly\n        # The equation shows sum over i,j of -log(numerator/denominator) where numerator corresponds to Y_i,j\n\n        # Assuming Y_i,j is 1 for the target class at that pixel and 0 otherwise.\n        # This is essentially -log(Pt(Xi,j)) for the correct class, summed over all pixels.\n        ldce_loss = -torch.log(pt_values + 1e-8) # Add small epsilon for stability\n        ldce_loss = torch.sum(ldce_loss) # Or torch.sum() depending on how you want to aggregate\n\n        Lvl_loss = torch.sum(difference_numerator.pow(2))\n        \n        \n        return ldce_loss + self.lambda_weight * Lvl_loss\n       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:59:43.329600Z","iopub.execute_input":"2025-10-11T15:59:43.329793Z","iopub.status.idle":"2025-10-11T15:59:43.348034Z","shell.execute_reply.started":"2025-10-11T15:59:43.329777Z","shell.execute_reply":"2025-10-11T15:59:43.347453Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"shape_resize = (512, 896)\n\nif COMPUTE_MEAN_STD:\n    mean_streethazards, std_streethazards = compute_mean_std_channels(StreetHazardsDataset(odgt_file= train_odgt_file,\n                                                                                           image_resize = shape_resize,\n                                                                                           spatial_transforms=None,\n                                                                                           mean_std=None))\nelse:\n    mean_streethazards, std_streethazards = [0.3302, 0.3459, 0.373], [0.1595, 0.1577, 0.1712]\n\nspatial_transforms = transforms.v2.Compose([\n    transforms.v2.RandomHorizontalFlip(),\n])\n\ntrain_dataset = StreetHazardsDataset(\n    odgt_file= train_odgt_file,\n    image_resize = shape_resize,\n    spatial_transforms=spatial_transforms,\n    mean_std=(mean_streethazards, std_streethazards)\n)\n\nval_dataset = StreetHazardsDataset(\n    odgt_file= val_odgt_file,\n    image_resize = shape_resize,\n    spatial_transforms=None,\n    mean_std=(mean_streethazards, std_streethazards)\n)\n\ntest_dataset = StreetHazardsDataset(\n    odgt_file= test_odgt_file,\n    image_resize = shape_resize,\n    spatial_transforms=None,\n    mean_std=(mean_streethazards, std_streethazards)\n)\n\ntrain_dl = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_dl = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\ntest_dl = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:59:43.348718Z","iopub.execute_input":"2025-10-11T15:59:43.348926Z","iopub.status.idle":"2025-10-11T15:59:43.458423Z","shell.execute_reply.started":"2025-10-11T15:59:43.348902Z","shell.execute_reply":"2025-10-11T15:59:43.457637Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"torch.cuda.empty_cache()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Esempio di utilizzo:\nencoder_name = \"resnet18\"\nencoder_weights = \"imagenet\"\nnum_known_classes = 13 # Numero di classi per cui i prototipi sono one-hot\nt_value = 3.0\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Inizializza il feature extractor\nfeature_extractor = DMLNetFeatureExtractor(\n    encoder_name=encoder_name,\n    encoder_weights=encoder_weights,\n    num_feature_channels=num_known_classes,\n    activation=None\n).to(device)\nmodel_optimizer = torch.optim.Adam(feature_extractor.parameters(), lr=0.001)\n\ndef train(num_epochs,model,train_loader,lambda_weight) -> None:\n        \n        for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n            \n            model.train()\n\n            losses = []\n\n            for batch in train_loader: \n\n                    \n                imgs = batch['image'].to(device)\n                labels = batch['labels'].to(device)\n                \n                logits = model(imgs)\n                \n                lossClass = DiscriminativeCrossEntropyLoss(Prototype,lambda_weight)\n\n                \n                loss = lossClass(logits,labels)\n                            \n                losses.append(loss.item())\n                \n                model_optimizer.zero_grad()\n                loss.backward()\n                model_optimizer.step()\n            \n                del loss\n                \n\n            l = sum(losses) / len(losses)\n\n            print(f\"Epoch {epoch + 1}\", end = ' ')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:59:43.460599Z","iopub.execute_input":"2025-10-11T15:59:43.460791Z","iopub.status.idle":"2025-10-11T15:59:45.777295Z","shell.execute_reply.started":"2025-10-11T15:59:43.460777Z","shell.execute_reply":"2025-10-11T15:59:45.776703Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7ffc7216de43d8b9842098de730151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6882be25122848cf80d5f45be1ba5881"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train(10,feature_extractor,train_dl,0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:51:50.754230Z","iopub.execute_input":"2025-10-11T14:51:50.754466Z","iopub.status.idle":"2025-10-11T15:37:42.061237Z","shell.execute_reply.started":"2025-10-11T14:51:50.754449Z","shell.execute_reply":"2025-10-11T15:37:42.060357Z"}},"outputs":[{"name":"stderr","text":"Epoch:  10%|█         | 1/10 [04:35<41:21, 275.77s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1 ","output_type":"stream"},{"name":"stderr","text":"Epoch:  20%|██        | 2/10 [09:10<36:43, 275.42s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2 ","output_type":"stream"},{"name":"stderr","text":"Epoch:  30%|███       | 3/10 [13:45<32:06, 275.22s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3 ","output_type":"stream"},{"name":"stderr","text":"Epoch:  40%|████      | 4/10 [18:21<27:31, 275.18s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4 ","output_type":"stream"},{"name":"stderr","text":"Epoch:  50%|█████     | 5/10 [22:56<22:55, 275.12s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 ","output_type":"stream"},{"name":"stderr","text":"Epoch:  60%|██████    | 6/10 [27:31<18:20, 275.11s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 6 ","output_type":"stream"},{"name":"stderr","text":"Epoch:  70%|███████   | 7/10 [32:06<13:45, 275.11s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 7 ","output_type":"stream"},{"name":"stderr","text":"Epoch:  80%|████████  | 8/10 [36:41<09:10, 275.05s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 8 ","output_type":"stream"},{"name":"stderr","text":"Epoch:  90%|█████████ | 9/10 [41:16<04:35, 275.05s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 9 ","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|██████████| 10/10 [45:51<00:00, 275.13s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 ","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"torch.save(feature_extractor, \"model.weights.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T15:38:33.710460Z","iopub.execute_input":"2025-10-11T15:38:33.711119Z","iopub.status.idle":"2025-10-11T15:38:33.811743Z","shell.execute_reply.started":"2025-10-11T15:38:33.711088Z","shell.execute_reply":"2025-10-11T15:38:33.811190Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"loaded_model = torch.load(\"/kaggle/input/dml/pytorch/default/1/model.weights.pth\",weights_only=False)\nloaded_model.eval()\nfeature_extractor = loaded_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T16:57:21.743861Z","iopub.execute_input":"2025-10-11T16:57:21.744461Z","iopub.status.idle":"2025-10-11T16:57:21.828517Z","shell.execute_reply.started":"2025-10-11T16:57:21.744432Z","shell.execute_reply":"2025-10-11T16:57:21.827905Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"def get_probability(logits):\n    # pixel_features: (B, D, H, W)\n    # target_labels: (B, H, W)\n\n    # Reshape pixel_features for easier broadcasting with prototypes\n    # (B, D, H, W) -> (B, H, W, D)\n    pixel_features_reshaped = logits.permute(0, 2, 3, 1) # (B, H, W, D)\n\n    probability = torch.clone(pixel_features_reshaped)\n    for t in range(13):\n        # Expand target_labels to match the feature dimension for indexing prototypes\n        # (B, H, W) -> (B, H, W, D)\n        prototypes_t = Prototype[t].to(device) # This should now be (B, H, W, D)\n    \n        # --- Numerator (Attractive Force) ---\n        # ||f(X; θf)i,j – mY i,j ||²\n        # (B, H, W, D) - (B, H, W, D) -> (B, H, W, D)\n        difference_numerator = pixel_features_reshaped - prototypes_t\n        # (B, H, W, D) -> (B, H, W)\n        squared_diff_numerator = torch.sum(difference_numerator.pow(2), dim=-1)\n        exp_squared_diff_numerator = torch.exp(-squared_diff_numerator)\n    \n        # --- Denominator (Repulsive Force) ---\n        # Σk=1 to N ( exp(-||f(X; θf)i,j – mk ||²) )\n        # To compute this, we need to calculate the squared difference for ALL prototypes\n        # and then sum their exponentials.\n    \n        # Expand pixel_features to compare with all prototypes: (B, H, W, 1, D)\n        pixel_features_expanded = pixel_features_reshaped.unsqueeze(-2) # (B, H, W, 1, D)\n    \n        # Expand prototypes to compare with all pixels: (1, 1, 1, N, D)\n        # self.prototypes has shape (N, D)\n        prototypes_expanded = prototypes_t.unsqueeze(0).unsqueeze(0).unsqueeze(0) # (1, 1, 1, N, D)\n    \n        # Calculate difference between each pixel feature and ALL prototypes\n        # (B, H, W, 1, D) - (1, 1, 1, N, D) -> (B, H, W, N, D)\n        all_prototypes_differences = pixel_features_expanded - prototypes_expanded\n    \n        # Square and sum across the feature dimension (D)\n        # (B, H, W, N, D) -> (B, H, W, N)\n        squared_diff_all_prototypes = torch.sum(all_prototypes_differences.pow(2), dim=-1)\n    \n        # Exponentiate\n        # (B, H, W, N)\n        exp_squared_diff_all_prototypes = torch.exp(-squared_diff_all_prototypes)\n    \n        # Sum across the prototype dimension (N) to get the denominator\n        # (B, H, W, N) -> (B, H, W)\n        denominator = torch.sum(exp_squared_diff_all_prototypes, dim=-1)\n    \n        # --- Calculate Pt(Xi,j) (Equation 2) ---\n        # (B, H, W) / (B, H, W) -> (B, H, W)\n        pt_values = exp_squared_diff_numerator / (denominator + 1e-8) # Add small epsilon for stability\n\n        probability[:,:,:,t] = pt_values\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T16:57:22.802220Z","iopub.execute_input":"2025-10-11T16:57:22.802911Z","iopub.status.idle":"2025-10-11T16:57:22.809434Z","shell.execute_reply.started":"2025-10-11T16:57:22.802887Z","shell.execute_reply":"2025-10-11T16:57:22.808590Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"for batch in test_dl:\n\n    imgs = batch['image'].to(device)\n    labels = batch['labels'].to(device)\n                \n    logits = feature_extractor(imgs)\n    probability_vector = get_probability(logits)\n\n    fig, axs = plt.subplots(1, 2, figsize=(10, 12))\n    idx = 2\n    visualize_scene(imgs.cpu()[0], axs[0])\n    visualize_annotation(probability_vector.cpu()[0], axs[1])\n    \n    break\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T16:57:25.117483Z","iopub.execute_input":"2025-10-11T16:57:25.117753Z","iopub.status.idle":"2025-10-11T16:57:26.511600Z","shell.execute_reply.started":"2025-10-11T16:57:25.117734Z","shell.execute_reply":"2025-10-11T16:57:26.510527Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1731856431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprobability_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/305969986.py\u001b[0m in \u001b[0;36mget_probability\u001b[0;34m(logits)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdifference_numerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_features_reshaped\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprototypes_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# (B, H, W, D) -> (B, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msquared_diff_numerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdifference_numerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mexp_squared_diff_numerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msquared_diff_numerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 207.12 MiB is free. Process 2523 has 15.68 GiB memory in use. Of the allocated memory 15.34 GiB is allocated by PyTorch, and 59.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 182.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 207.12 MiB is free. Process 2523 has 15.68 GiB memory in use. Of the allocated memory 15.34 GiB is allocated by PyTorch, and 59.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":79},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}