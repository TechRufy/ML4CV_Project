{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":614668,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":453419,"modelId":469716}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DATA_DIR = \"./data\"\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:04:43.054229Z","iopub.execute_input":"2025-10-20T18:04:43.054724Z","iopub.status.idle":"2025-10-20T18:04:43.061024Z","shell.execute_reply.started":"2025-10-20T18:04:43.054703Z","shell.execute_reply":"2025-10-20T18:04:43.060272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p $DATA_DIR\n!test ! -d $DATA_DIR/train \\\n    && wget -O $DATA_DIR/train.tar https://people.eecs.berkeley.edu/~hendrycks/streethazards_train.tar \\\n    && tar -xf $DATA_DIR/train.tar -C $DATA_DIR \\\n    && rm -r $DATA_DIR/train.tar \\\n    && mv $DATA_DIR/train $DATA_DIR/streethazards_train\n!test ! -d $DATA_DIR/test \\\n    && wget -O $DATA_DIR/test.tar https://people.eecs.berkeley.edu/~hendrycks/streethazards_test.tar \\\n    && tar -xf $DATA_DIR/test.tar -C $DATA_DIR \\\n    && rm -r $DATA_DIR/test.tar\\\n    && mv $DATA_DIR/test $DATA_DIR/streethazards_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:04:43.966479Z","iopub.execute_input":"2025-10-20T18:04:43.967045Z","iopub.status.idle":"2025-10-20T18:08:05.329758Z","shell.execute_reply.started":"2025-10-20T18:04:43.967022Z","shell.execute_reply":"2025-10-20T18:08:05.328781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:08:05.331433Z","iopub.execute_input":"2025-10-20T18:08:05.331658Z","iopub.status.idle":"2025-10-20T18:09:28.138225Z","shell.execute_reply.started":"2025-10-20T18:08:05.331639Z","shell.execute_reply":"2025-10-20T18:09:28.137437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom enum import IntEnum\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport segmentation_models_pytorch as smp\nfrom typing import Optional, Callable, Union, Tuple, Dict, List\nimport json\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torchvision import transforms\nfrom torchvision.transforms import v2\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nknown_classes = 13","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:09:28.139138Z","iopub.execute_input":"2025-10-20T18:09:28.139332Z","iopub.status.idle":"2025-10-20T18:09:39.168595Z","shell.execute_reply.started":"2025-10-20T18:09:28.139313Z","shell.execute_reply":"2025-10-20T18:09:39.167765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nSource: https://github.com/hendrycks/anomaly-seg/issues/15#issuecomment-890300278\n\"\"\"\nCOLORS = np.array([\n[  0,   0,   0], # // unlabeled     =   0, black\n[ 70,  70,  70], # // building      =   1, gray\n[190, 153, 153], # // fence         =   2, pinkish\n[250, 170, 160], # // other         =   3, pink\n[220,  20,  60], # // pedestrian    =   4, \n[153, 153, 153], # // pole          =   5,\n[157, 234,  50], # // road line     =   6, \n[128,  64, 128], # // road          =   7,\n[244,  35, 232], # // sidewalk      =   8,\n[107, 142,  35], # // vegetation    =   9, \n[  0,   0, 142], # // car           =  10,\n[102, 102, 156], # // wall          =  11, \n[220, 220,   0], # // traffic sign  =  12,\n[ 60, 250, 240], # // anomaly       =  13,\n]) \n\n\ndef visualize_annotation(annotation_img: np.ndarray | torch.Tensor, ax=None, title= None) -> None:\n    \"\"\"\n    Visualize a segmentation annotation using a predefined color palette.\n\n    Args:\n        annotation_img (np.ndarray | torch.Tensor): 2D array with class indices.\n        ax (matplotlib.axes.Axes, optional): Axis to plot on. If None, uses current axis.\n    \"\"\"\n    if ax is None: ax = plt.gca()\n    annotation_img = np.asarray(annotation_img)\n    img_new = np.zeros((*annotation_img.shape, 3))\n\n    for index, color in enumerate(COLORS):\n        img_new[annotation_img == index] = color\n\n    ax.imshow(img_new / 255.0)\n    if title:\n        ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\ndef visualize_scene(image: np.ndarray | torch.Tensor, ax=None, title= None) -> None:\n    \"\"\"\n    Visualize a raw RGB scene image.\n\n    Args:\n        image (np.ndarray | torch.Tensor): Image tensor or array in [C, H, W] or [H, W, C] format.\n        ax (matplotlib.axes.Axes, optional): Axis to plot on. If None, uses current axis.\n    \"\"\"\n    if ax is None: ax = plt.gca()\n    image = np.asarray(image)\n    ax.imshow(np.moveaxis(image, 0, -1))\n    if title:\n        ax.set_title(title)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    \n#path to streethazards dataset\ntrain_odgt_file = f\"{DATA_DIR}/streethazards_train/train.odgt\"\nval_odgt_file = f\"{DATA_DIR}/streethazards_train/validation.odgt\"\ntest_odgt_file = f\"{DATA_DIR}/streethazards_test/test.odgt\"\n\nCOMPUTE_MEAN_STD = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:09:39.170142Z","iopub.execute_input":"2025-10-20T18:09:39.170350Z","iopub.status.idle":"2025-10-20T18:09:39.178190Z","shell.execute_reply.started":"2025-10-20T18:09:39.170334Z","shell.execute_reply":"2025-10-20T18:09:39.177338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StreetHazardsDataset(Dataset):\n    \"\"\"\n    A custom PyTorch Dataset for the StreetHazards inliner dataset.\n\n    This dataset reads image and segmentation label paths from a `.odgt` file,\n    applies optional resizing and spatial transformations, and returns\n    dictionary-style samples with normalized image tensors and label tensors.\n\n    Args:\n        odgt_file (str): Path to the `.odgt` file containing image and label metadata.\n        image_resize (Tuple[int, int], optional): Target size to resize images and labels. \n        spatial_transforms (Callable, optional): Optional transformation function applied to both images and labels.\n        mean_std (Tuple[List[float], List[float]], optional): Mean and standard deviation for image normalization.\n        \n    \"\"\"\n    def __init__(\n        self,\n        odgt_file: str,\n        image_resize: Tuple[int, int] = (512, 896),\n        spatial_transforms: Optional[Callable] = None,\n        mean_std: Tuple[List[float], List[float]] = None\n    ):\n\n        self.spatial_transforms = spatial_transforms\n        self.mean_std = mean_std\n        self.image_resize = image_resize\n\n        with open(odgt_file, \"r\") as f:\n            odgt_data = json.load(f)\n        \n\n        self.paths = [\n            {\n                \"image\": os.path.join(Path(odgt_file).parent, data[\"fpath_img\"]),\n                \"labels\": os.path.join(Path(odgt_file).parent, data[\"fpath_segm\"]),\n            }\n            for data in odgt_data \n        ]\n    \n    def __len__(self) -> int:\n        return len(self.paths)\n\n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n\n        image = Image.open(self.paths[idx][\"image\"]).convert(\"RGB\")\n        labels = Image.open(self.paths[idx][\"labels\"])\n\n        if self.image_resize:\n            image = transforms.Resize(self.image_resize, transforms.InterpolationMode.BILINEAR)(image)\n            labels = transforms.Resize(self.image_resize, transforms.InterpolationMode.NEAREST)(labels)\n            \n        if self.spatial_transforms:\n            image, labels  = self.spatial_transforms(image, labels)         \n\n        #to_tensor\n        image = transforms.ToTensor()(image)\n        labels = torch.as_tensor(transforms.functional.pil_to_tensor(labels), dtype=torch.int64) - 1\n        \n        labels = labels.squeeze(0)\n        \n        if self.mean_std:\n            image = transforms.Normalize(mean = self.mean_std[0], std = self.mean_std[1])(image)\n\n        return {'image' : image, 'labels' : labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:09:39.178794Z","iopub.execute_input":"2025-10-20T18:09:39.178953Z","iopub.status.idle":"2025-10-20T18:09:39.195684Z","shell.execute_reply.started":"2025-10-20T18:09:39.178940Z","shell.execute_reply":"2025-10-20T18:09:39.194936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_one_hot_prototypes_torch(num_known_classes: int, t_value: float = 3.0, device: str = 'cpu') -> torch.Tensor:\n    \"\"\"\n    Generates one-hot prototypes as a PyTorch tensor for a given number of known classes.\n    Each prototype is a vector where only the element corresponding\n    to its class index has the 't_value', and all other elements are 0.\n\n    Args:\n        num_known_classes (int): The total number of known (in-distribution) classes.\n                                 This also determines the dimensionality of each prototype vector.\n        t_value (float): The non-zero value at the class's specific index in the prototype.\n                         As specified in the paper, this is often 3.0.\n        device (str): The device on which to create the tensor ('cpu' or 'cuda').\n\n    Returns:\n        torch.Tensor: A 2D PyTorch tensor where each row is a prototype vector.\n                      The shape will be (num_known_classes, num_known_classes).\n    \"\"\"\n    if not isinstance(num_known_classes, int) or num_known_classes <= 0:\n        raise ValueError(\"num_known_classes must be a positive integer.\")\n    if not isinstance(t_value, (int, float)):\n        raise ValueError(\"t_value must be a numeric type.\")\n    if device not in ['cpu', 'cuda']:\n        raise ValueError(\"device must be 'cpu' or 'cuda'.\")\n\n    # Create a tensor of zeros\n    prototypes = torch.zeros((num_known_classes, num_known_classes), dtype=torch.float32, device=device)\n\n    # Fill the diagonal with t_value to create one-hot prototypes\n    for i in range(num_known_classes):\n        prototypes[i, i] = t_value\n        \n    # An even more concise way using torch.eye (Identity matrix)\n    # prototypes = torch.eye(num_known_classes, dtype=torch.float32, device=device) * t_value\n\n    return prototypes\n\nPrototype  = create_one_hot_prototypes_torch(known_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:09:39.196324Z","iopub.execute_input":"2025-10-20T18:09:39.196545Z","iopub.status.idle":"2025-10-20T18:09:39.219293Z","shell.execute_reply.started":"2025-10-20T18:09:39.196521Z","shell.execute_reply":"2025-10-20T18:09:39.218560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass DMLNetFeatureExtractor(torch.nn.Module):\n    def __init__(self, encoder_name, encoder_weights, num_feature_channels, activation):\n        super().__init__()\n        \n\n        self.model = smp.DeepLabV3Plus(\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            classes=num_feature_channels, # This sets the output channels of the segmentation_head if kept\n            activation=activation # Usually 'None' for the main head, but for features it might not matter directly\n        )\n        \n\n        # Option 2 is safer and more robust.\n        # First, we disable the original segmentation head as you did.\n        self.original_segmentation_head = self.model.segmentation_head # Store it if needed\n        self.model.segmentation_head = torch.nn.Identity() # Remove the final head\n\n        # --- CORRECTION START ---\n        # To get the decoder's actual output channels, we need a dummy forward pass\n        # through just the encoder and decoder.\n        \n        # Temporarily detach the module to make a dummy pass if needed,\n        # but in __init__, we can usually just do a conceptual forward.\n        # However, to be absolutely safe and get the runtime channel count:\n        \n        # Create a dummy input to trace the decoder output channels\n        # Assuming typical RGB input (3 channels) and arbitrary spatial dimensions\n        dummy_input = torch.randn(2, 3, 256, 256) \n        \n        # Pass through encoder\n        encoder_features_dummy = self.model.encoder(dummy_input)\n        \n        # Pass through decoder to get its output channels\n        decoder_output_dummy = self.model.decoder(encoder_features_dummy)\n        \n        # Extract the channel dimension from the dummy output\n        decoder_actual_out_channels = decoder_output_dummy.shape[1]\n        # --- CORRECTION END ---\n\n        # Add a 1x1 convolution to project the decoder's output to the desired num_feature_channels.\n        self.feature_projection = torch.nn.Conv2d(\n            in_channels=decoder_actual_out_channels,\n            out_channels=num_feature_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0\n        )\n        \n    def forward(self, x):\n        input_spatial_size = x.shape[2:]\n        # The encoder outputs a list of feature maps at different resolutions\n        encoder_features = self.model.encoder(x)\n        \n        # The decoder takes these features and produces a high-resolution feature map.\n        # This output will typically have the same spatial dimensions as the input 'x'\n        # (due to DeepLabV3+ decoder's upsampling) but with its default channel count.\n        decoder_output = self.model.decoder(encoder_features)\n        \n        # Project the decoder's output to the desired number of feature channels\n        projected_features = self.feature_projection(decoder_output)\n\n        final_features = torch.nn.functional.interpolate(\n            projected_features, \n            size=input_spatial_size, \n            mode='bilinear', \n            align_corners=False # Set to True for pixel alignment if needed, but False is common\n        )\n        \n        # These `final_features` are your f(X; θf)i,j with num_feature_channels.\n        return final_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:09:39.220111Z","iopub.execute_input":"2025-10-20T18:09:39.220513Z","iopub.status.idle":"2025-10-20T18:09:39.227196Z","shell.execute_reply.started":"2025-10-20T18:09:39.220489Z","shell.execute_reply":"2025-10-20T18:09:39.226605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DiscriminativeCrossEntropyLoss(nn.Module):\n    def __init__(self, prototypes: torch.Tensor,lambda_weight):\n        super().__init__()\n       \n        self.prototypes = prototypes.to(device)\n        self.lambda_weight = lambda_weight\n\n    def forward(self, pixel_features: torch.Tensor, target_labels: torch.Tensor):\n        # pixel_features: (B, D, H, W)\n        # target_labels: (B, H, W)\n\n        # Reshape pixel_features for easier broadcasting with prototypes\n        # (B, D, H, W) -> (B, H, W, D)\n        pixel_features_reshaped = pixel_features.permute(0, 2, 3, 1) # (B, H, W, D)\n\n        # Expand target_labels to match the feature dimension for indexing prototypes\n        # (B, H, W) -> (B, H, W, D)\n        prototypes_target = self.prototypes[target_labels] # This should now be (B, H, W, D)\n\n        # --- Numerator (Attractive Force) ---\n        # ||f(X; θf)i,j – mY i,j ||²\n        # (B, H, W, D) - (B, H, W, D) -> (B, H, W, D)\n        difference_numerator = pixel_features_reshaped - prototypes_target\n        # (B, H, W, D) -> (B, H, W)\n        squared_diff_numerator = torch.sum(difference_numerator.pow(2), dim=-1)\n        exp_squared_diff_numerator = torch.exp(-squared_diff_numerator)\n\n        # --- Denominator (Repulsive Force) ---\n        # Σk=1 to N ( exp(-||f(X; θf)i,j – mk ||²) )\n        # To compute this, we need to calculate the squared difference for ALL prototypes\n        # and then sum their exponentials.\n\n        # Expand pixel_features to compare with all prototypes: (B, H, W, 1, D)\n        pixel_features_expanded = pixel_features_reshaped.unsqueeze(-2) # (B, H, W, 1, D)\n\n        # Expand prototypes to compare with all pixels: (1, 1, 1, N, D)\n        # self.prototypes has shape (N, D)\n        prototypes_expanded = self.prototypes.unsqueeze(0).unsqueeze(0).unsqueeze(0) # (1, 1, 1, N, D)\n\n        # Calculate difference between each pixel feature and ALL prototypes\n        # (B, H, W, 1, D) - (1, 1, 1, N, D) -> (B, H, W, N, D)\n        all_prototypes_differences = pixel_features_expanded - prototypes_expanded\n\n        # Square and sum across the feature dimension (D)\n        # (B, H, W, N, D) -> (B, H, W, N)\n        squared_diff_all_prototypes = torch.sum(all_prototypes_differences.pow(2), dim=-1)\n\n        # Exponentiate\n        # (B, H, W, N)\n        exp_squared_diff_all_prototypes = torch.exp(-squared_diff_all_prototypes)\n\n        # Sum across the prototype dimension (N) to get the denominator\n        # (B, H, W, N) -> (B, H, W)\n        denominator = torch.sum(exp_squared_diff_all_prototypes, dim=-1)\n\n        # --- Calculate Pt(Xi,j) (Equation 2) ---\n        # (B, H, W) / (B, H, W) -> (B, H, W)\n        pt_values = exp_squared_diff_numerator / (denominator + 1e-8) # Add small epsilon for stability\n\n        # --- Calculate LDCE (Equation 3) ---\n        # LDCE = -log(pt_values) for target classes\n        # This requires masking based on target_labels or using the pt_values directly\n        # The equation shows sum over i,j of -log(numerator/denominator) where numerator corresponds to Y_i,j\n\n        # Assuming Y_i,j is 1 for the target class at that pixel and 0 otherwise.\n        # This is essentially -log(Pt(Xi,j)) for the correct class, summed over all pixels.\n        ldce_loss = -torch.log(pt_values + 1e-8) # Add small epsilon for stability\n        ldce_loss = torch.sum(ldce_loss) # Or torch.sum() depending on how you want to aggregate\n\n        Lvl_loss = torch.sum(difference_numerator.pow(2))\n        \n        # Check for NaNs/Infs\n        if torch.isnan(pixel_features_reshaped).any():\n            print(\"NaN found in pixel_features_reshaped!\")\n        if torch.isinf(pixel_features_reshaped).any():\n            print(\"Inf found in pixel_features_reshaped!\")\n        \n        \n        return ldce_loss + self.lambda_weight * Lvl_loss\n       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:09:39.227874Z","iopub.execute_input":"2025-10-20T18:09:39.228497Z","iopub.status.idle":"2025-10-20T18:09:39.243972Z","shell.execute_reply.started":"2025-10-20T18:09:39.228479Z","shell.execute_reply":"2025-10-20T18:09:39.243411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shape_resize = (512, 896)\n\nspatial_transforms = transforms.v2.Compose([\n    transforms.v2.RandomHorizontalFlip(),\n])\n\ntrain_dataset = StreetHazardsDataset(\n    odgt_file= train_odgt_file\n)\n\nval_dataset = StreetHazardsDataset(\n    odgt_file= val_odgt_file\n)\n\ntest_dataset = StreetHazardsDataset(\n    odgt_file= test_odgt_file\n)\n\ntrain_dl = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_dl = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\ntest_dl = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Esempio di utilizzo:\nencoder_name = \"efficientnet-b2\"\nencoder_weights = \"imagenet\"\nt_value = 3.0\nepochs = 100\n\n# Inizializza il feature extractor\nfeature_extractor = DMLNetFeatureExtractor(\n    encoder_name=encoder_name,\n    encoder_weights=encoder_weights,\n    num_feature_channels=known_classes,\n    activation=None\n).to(device)\nmodel_optimizer = torch.optim.Adam(feature_extractor.parameters(), lr=0.02)\n\ndef train(num_epochs,model,train_loader,lambda_weight) -> None:\n        \n        for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n            \n            model.train()\n\n            losses = []\n\n            for batch in train_loader: \n\n                    \n                imgs = batch['image'].to(device)\n                labels = batch['labels'].to(device)\n                \n                logits = model(imgs)\n                \n                lossClass = DiscriminativeCrossEntropyLoss(Prototype,lambda_weight)\n\n                \n                loss = lossClass(logits,labels)\n                            \n                losses.append(loss.item())\n                \n                model_optimizer.zero_grad()\n                loss.backward()\n                model_optimizer.step()\n            \n                del loss\n                \n\n            l = sum(losses) / len(losses)\n\n            print(f\"Epoch {epoch + 1} Loss {l}\", end = ' ')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:09:39.362169Z","iopub.execute_input":"2025-10-20T18:09:39.362374Z","iopub.status.idle":"2025-10-20T18:09:41.799880Z","shell.execute_reply.started":"2025-10-20T18:09:39.362359Z","shell.execute_reply":"2025-10-20T18:09:41.799274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loaded_model = torch.load(\"/kaggle/input/dml/pytorch/default/10/model_weights10E_resnet34.pth\",weights_only=False)\n#feature_extractor = loaded_model\n#print(\"model loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:53:36.818866Z","iopub.execute_input":"2025-10-20T17:53:36.819677Z","iopub.status.idle":"2025-10-20T17:53:37.745910Z","shell.execute_reply.started":"2025-10-20T17:53:36.819645Z","shell.execute_reply":"2025-10-20T17:53:37.745257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(epochs,feature_extractor,train_dl,0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:09:41.800566Z","iopub.execute_input":"2025-10-20T18:09:41.800758Z","iopub.status.idle":"2025-10-20T18:12:34.270683Z","shell.execute_reply.started":"2025-10-20T18:09:41.800743Z","shell.execute_reply":"2025-10-20T18:12:34.269472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(feature_extractor, \"model_weights100E_efficientnet-b4.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T18:12:34.271327Z","iopub.status.idle":"2025-10-20T18:12:34.271549Z","shell.execute_reply.started":"2025-10-20T18:12:34.271443Z","shell.execute_reply":"2025-10-20T18:12:34.271452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loaded_model = torch.load(\"/kaggle/input/dml/pytorch/default/4/model.weights.pth\",weights_only=False)\n#loaded_model.eval()\n#feature_extractor = loaded_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:16:49.292075Z","iopub.execute_input":"2025-10-20T17:16:49.292350Z","iopub.status.idle":"2025-10-20T17:16:49.308545Z","shell.execute_reply.started":"2025-10-20T17:16:49.292313Z","shell.execute_reply":"2025-10-20T17:16:49.307708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_probability(logits, Prototype, device): # Passa Prototype e device come argomenti\n    # logits: (B, D, H, W)\n    # Prototype: (N_CLASSES, D)  -> N_CLASSES dovrebbe essere 12 nel tuo caso\n    \n    # Reshape logits for easier broadcasting with prototypes\n    # (B, D, H, W) -> (B, H, W, D)\n    pixel_features_reshaped = logits.permute(0, 2, 3, 1) # (B, H, W, D)\n    \n    B, H, W, D = pixel_features_reshaped.shape\n    N_CLASSES = Prototype.shape[0] # Numero di classi/prototipi, dovrebbe essere 12\n    \n    # Inizializza il tensore 'probability' con zeri. Questo è il tensore di output finale.\n    probability = torch.zeros(B, H, W, N_CLASSES, device=device)\n\n    # Sposta Prototype sulla device una sola volta, se non è già lì\n    Prototypes_on_device = Prototype.to(device)\n\n    # --- Pre-calcolo delle differenze e delle somme per il denominatore (più efficiente) ---\n    # Per il denominatore, abbiamo bisogno di calcolare le distanze di ogni pixel_feature da *tutti* i prototipi.\n    # Questo può essere fatto in modo vettoriale, senza un loop sui prototipi per questa parte.\n\n    # Expand pixel_features_reshaped to compare with all prototypes: (B, H, W, 1, D)\n    pixel_features_expanded_for_denom = pixel_features_reshaped.unsqueeze(-2) \n    \n    # Expand Prototypes_on_device to compare with all pixels: (1, 1, 1, N_CLASSES, D)\n    # (N_CLASSES, D) -> (1, 1, 1, N_CLASSES, D)\n    prototypes_expanded_for_denom = Prototypes_on_device.unsqueeze(0).unsqueeze(0).unsqueeze(0) \n\n    # Calculate difference between each pixel feature and ALL prototypes\n    # (B, H, W, 1, D) - (1, 1, 1, N_CLASSES, D) -> (B, H, W, N_CLASSES, D)\n    all_prototypes_differences = pixel_features_expanded_for_denom - prototypes_expanded_for_denom\n    \n    # Square and sum across the feature dimension (D)\n    # (B, H, W, N_CLASSES, D) -> (B, H, W, N_CLASSES)\n    squared_diff_all_prototypes = torch.sum(all_prototypes_differences.pow(2), dim=-1)\n    \n    # Exponentiate\n    # (B, H, W, N_CLASSES)\n    exp_squared_diff_all_prototypes = torch.exp(-squared_diff_all_prototypes)\n    \n    # Sum across the prototype dimension (N_CLASSES) to get the denominator for *all* classes\n    # (B, H, W, N_CLASSES) -> (B, H, W)\n    denominator = torch.sum(exp_squared_diff_all_prototypes, dim=-1, keepdim=True) + 1e-8 # keepdim per broadcasting\n\n    # Ora loop per calcolare il numeratore e la probabilità per ogni classe 't'\n    for t in range(N_CLASSES):\n        # prototypes_t è il prototipo specifico per la classe 't': (D,)\n        prototypes_t = Prototypes_on_device[t] \n    \n        # --- Numerator (Attractive Force) ---\n        # ||f(X; θf)i,j – m_t ||²  (m_t è il prototipo per la classe t)\n        # (B, H, W, D) - (D) -> (B, H, W, D) (broadcasting)\n        difference_numerator = pixel_features_reshaped - prototypes_t\n        \n        # (B, H, W, D) -> (B, H, W)\n        squared_diff_numerator = torch.sum(difference_numerator.pow(2), dim=-1)\n        exp_squared_diff_numerator = torch.exp(-squared_diff_numerator)\n    \n        # --- Calculate Pt(Xi,j) (Equation 2) ---\n        # (B, H, W) / (B, H, W) -> (B, H, W)\n        pt_values = exp_squared_diff_numerator / denominator.squeeze(-1) # Rimuovi la dimensione aggiunta per il broadcasting\n        \n        # Assegna i valori di probabilità per la classe 't'\n        probability[:,:,:,t] = pt_values\n    \n    # Restituisci il tensore completo delle probabilità dopo il ciclo\n    return probability","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:16:49.309436Z","iopub.execute_input":"2025-10-20T17:16:49.309696Z","iopub.status.idle":"2025-10-20T17:16:49.326120Z","shell.execute_reply.started":"2025-10-20T17:16:49.309677Z","shell.execute_reply":"2025-10-20T17:16:49.325140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.memory._record_memory_history()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:16:49.327005Z","iopub.execute_input":"2025-10-20T17:16:49.327252Z","iopub.status.idle":"2025-10-20T17:16:49.385964Z","shell.execute_reply.started":"2025-10-20T17:16:49.327234Z","shell.execute_reply":"2025-10-20T17:16:49.384932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_extractor.eval()\nwith torch.no_grad():\n    for batch in test_dl:\n    \n        imgs = batch['image'].to(device)\n        labels = batch['labels'].to(device)\n                    \n        logits = feature_extractor(imgs)\n        probability_vector = get_probability(logits,Prototype,device)\n    \n        fig, axs = plt.subplots(1, 3, figsize=(13, 13))\n        idx = 3\n        visualize_scene(imgs.cpu()[5], axs[0])\n        visualize_annotation(torch.argmax(probability_vector,axis=3).cpu().detach().numpy()[5], axs[1])\n        visualize_annotation(labels.cpu().detach().numpy()[5],axs[2])\n        \n        break\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:53:41.388452Z","iopub.execute_input":"2025-10-20T17:53:41.388763Z","iopub.status.idle":"2025-10-20T17:53:43.477149Z","shell.execute_reply.started":"2025-10-20T17:53:41.388741Z","shell.execute_reply":"2025-10-20T17:53:43.476149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def MMSP(probability_map):\n    MMSP_map = 1 - torch.max(probability_map, dim=3)[0] \n    return MMSP_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:16:52.151607Z","iopub.execute_input":"2025-10-20T17:16:52.151918Z","iopub.status.idle":"2025-10-20T17:16:52.157655Z","shell.execute_reply.started":"2025-10-20T17:16:52.151887Z","shell.execute_reply":"2025-10-20T17:16:52.156759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Probability_EDS(pixel_feature):\n    pixel_features_reshaped = pixel_feature.permute(0, 2, 3, 1) \n\n    S_xy = torch.zeros(pixel_features_reshaped.shape[0], pixel_features_reshaped.shape[1], pixel_features_reshaped.shape[2], device=pixel_feature.device)\n    \n    for t in range(known_classes):\n        # Assicurati che Prototype[t] abbia la forma corretta per il broadcasting\n        # Se Prototype è (known_classes, D), allora .view(1, 1, 1, -1) lo rende (1, 1, 1, D)\n        prototypes_t = Prototype[t].to(device).view(1, 1, 1, -1)\n        \n        difference_numerator = pixel_features_reshaped - prototypes_t\n        \n        # Somma dei quadrati lungo la dimensione D (ultima dimensione)\n        squared_diff_numerator = torch.sum(difference_numerator**2, dim=3)\n        S_xy = S_xy + squared_diff_numerator\n\n    MaxS = torch.max(S_xy) # torch.max su un tensore singolo restituisce un tensore\n    P_EDS = 1 - S_xy / (MaxS + 1e-8) # Aggiungi epsilon per stabilità numerica\n    \n    return P_EDS\n\n        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:16:52.158719Z","iopub.execute_input":"2025-10-20T17:16:52.159017Z","iopub.status.idle":"2025-10-20T17:16:52.603848Z","shell.execute_reply.started":"2025-10-20T17:16:52.158992Z","shell.execute_reply":"2025-10-20T17:16:52.602906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    for batch in test_dl:\n    \n        imgs = batch['image'].to(device)\n        labels = batch['labels'].to(device)\n                    \n        logits = feature_extractor(imgs)\n        probability_vector = get_probability(logits,Prototype,device)\n    \n        fig, axs = plt.subplots(1, 2, figsize=(10, 12))\n        idx = 2\n        \n        visualize_scene(np.rot90(MMSP(probability_vector)[5].cpu().detach().numpy()), axs[0])\n        visualize_scene(np.rot90(Probability_EDS(logits)[5].cpu().detach().numpy()), axs[1])\n        \n        break\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:53:02.779242Z","iopub.execute_input":"2025-10-20T17:53:02.780113Z","iopub.status.idle":"2025-10-20T17:53:04.584428Z","shell.execute_reply.started":"2025-10-20T17:53:02.780060Z","shell.execute_reply":"2025-10-20T17:53:04.583269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Anomaly_probability(MMSP_map, P_EDS_map, gamma, beta):\n\n    alpha = 1 / (1 + torch.exp(-beta * (P_EDS_map - gamma)))\n\n    Anomaly_p = alpha * P_EDS_map + (1 - alpha) * MMSP_map\n    return Anomaly_p\n\ndef Final_map(logits, lambda_value, gamma, beta):\n\n    probability_vector = get_probability(logits, Prototype, device) \n    vector_mmsp = MMSP(probability_vector) \n    vector_eds = Probability_EDS(logits) \n    Anomaly_map = Anomaly_probability(vector_mmsp, vector_eds, gamma, beta) # (B, H, W)\n    segmentation_map = torch.argmax(probability_vector, dim=3) # (B, H, W)\n    ood_label = 13 \n    final_map = torch.where(Anomaly_map <= lambda_value, segmentation_map, torch.full_like(segmentation_map, ood_label))\n\n    return final_map.cpu().numpy(), Anomaly_map.cpu().numpy()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:43:05.348298Z","iopub.execute_input":"2025-10-20T17:43:05.349034Z","iopub.status.idle":"2025-10-20T17:43:05.354916Z","shell.execute_reply.started":"2025-10-20T17:43:05.349009Z","shell.execute_reply":"2025-10-20T17:43:05.354128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lambda_value = 0.55\nwith torch.no_grad():\n    for batch in test_dl:\n    \n        imgs = batch['image'].to(device)\n        labels = batch['labels'].to(device)\n    \n        fig, axs = plt.subplots(1, 3, figsize=(10, 12))\n        idx = 2\n\n        visualize_annotation(Final_map(logits,lambda_value,0.5,20)[0][5], axs[0])\n        visualize_scene(Final_map(logits,lambda_value,0.5,20)[1][5], axs[1])\n        visualize_annotation(labels.cpu().detach().numpy()[5],axs[2])\n        \n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:53:16.788341Z","iopub.execute_input":"2025-10-20T17:53:16.788681Z","iopub.status.idle":"2025-10-20T17:53:18.916244Z","shell.execute_reply.started":"2025-10-20T17:53:16.788654Z","shell.execute_reply":"2025-10-20T17:53:18.915109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T17:56:33.757678Z","iopub.execute_input":"2025-10-20T17:56:33.758416Z","iopub.status.idle":"2025-10-20T17:56:35.226283Z","shell.execute_reply.started":"2025-10-20T17:56:33.758389Z","shell.execute_reply":"2025-10-20T17:56:35.225073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}