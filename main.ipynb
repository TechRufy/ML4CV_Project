{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e16d1f26-7862-4e7c-b721-6a95aa03c7cf","cell_type":"code","source":"DATA_DIR = \"./data\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T22:31:31.271523Z","iopub.execute_input":"2025-10-05T22:31:31.272259Z","iopub.status.idle":"2025-10-05T22:31:31.275519Z","shell.execute_reply.started":"2025-10-05T22:31:31.272227Z","shell.execute_reply":"2025-10-05T22:31:31.274918Z"}},"outputs":[],"execution_count":6},{"id":"ba697ea1-1879-4023-8113-c0ae3f4938f3","cell_type":"code","source":"\n!mkdir -p $DATA_DIR\n!test ! -d $DATA_DIR/train \\\n    && wget -O $DATA_DIR/train.tar https://people.eecs.berkeley.edu/~hendrycks/streethazards_train.tar \\\n    && tar -xf $DATA_DIR/train.tar -C $DATA_DIR \\\n    && rm -r $DATA_DIR/train.tar \\\n    && mv $DATA_DIR/train $DATA_DIR/streethazards_train\n!test ! -d $DATA_DIR/test \\\n    && wget -O $DATA_DIR/test.tar https://people.eecs.berkeley.edu/~hendrycks/streethazards_test.tar \\\n    && tar -xf $DATA_DIR/test.tar -C $DATA_DIR \\\n    && rm -r $DATA_DIR/test.tar\\\n    && mv $DATA_DIR/test $DATA_DIR/streethazards_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T20:55:44.869244Z","iopub.execute_input":"2025-10-05T20:55:44.869636Z","iopub.status.idle":"2025-10-05T21:00:24.074622Z","shell.execute_reply.started":"2025-10-05T20:55:44.869610Z","shell.execute_reply":"2025-10-05T21:00:24.073859Z"}},"outputs":[{"name":"stdout","text":"--2025-10-05 20:55:45--  https://people.eecs.berkeley.edu/~hendrycks/streethazards_train.tar\nResolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\nConnecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9386226176 (8.7G) [application/x-tar]\nSaving to: ‘./data/train.tar’\n\n./data/train.tar    100%[===================>]   8.74G  49.9MB/s    in 3m 16s  \n\n2025-10-05 20:59:01 (45.7 MB/s) - ‘./data/train.tar’ saved [9386226176/9386226176]\n\n--2025-10-05 20:59:25--  https://people.eecs.berkeley.edu/~hendrycks/streethazards_test.tar\nResolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\nConnecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2150484992 (2.0G) [application/x-tar]\nSaving to: ‘./data/test.tar’\n\n./data/test.tar     100%[===================>]   2.00G  21.2MB/s    in 56s     \n\n2025-10-05 21:00:21 (36.9 MB/s) - ‘./data/test.tar’ saved [2150484992/2150484992]\n\n","output_type":"stream"}],"execution_count":2},{"id":"ba6824836148648e","cell_type":"code","source":"!pip install -U segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T21:01:23.351350Z","iopub.execute_input":"2025-10-05T21:01:23.351629Z","iopub.status.idle":"2025-10-05T21:02:37.057700Z","shell.execute_reply.started":"2025-10-05T21:01:23.351608Z","shell.execute_reply":"2025-10-05T21:02:37.056958Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting segmentation-models-pytorch\n  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.33.1)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.26.4)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\nRequirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\nRequirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->segmentation-models-pytorch) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->segmentation-models-pytorch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation-models-pytorch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->segmentation-models-pytorch) (2024.2.0)\nDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation-models-pytorch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0\n","output_type":"stream"}],"execution_count":4},{"id":"2a260aec-96e0-4e2c-9e49-dcd94f9ebcc6","cell_type":"code","source":"import numpy as np\nimport os\nfrom enum import IntEnum\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport segmentation_models_pytorch as smp\nfrom typing import Optional, Callable, Union, Tuple, Dict, List\nimport json\nfrom pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom torchvision import transforms\nfrom torchvision.transforms import v2\nfrom tqdm import tqdm\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T22:31:58.908924Z","iopub.execute_input":"2025-10-05T22:31:58.909502Z","iopub.status.idle":"2025-10-05T22:31:58.913637Z","shell.execute_reply.started":"2025-10-05T22:31:58.909482Z","shell.execute_reply":"2025-10-05T22:31:58.913071Z"}},"outputs":[],"execution_count":16},{"id":"e34d47df-b5a0-4a00-a402-52ba7ee24f0e","cell_type":"code","source":"\"\"\"\nSource: https://github.com/hendrycks/anomaly-seg/issues/15#issuecomment-890300278\n\"\"\"\nCOLORS = np.array([\n    [ 70,  70,  70],  # building     =   0,\n    [190, 153, 153],  # fence        =   1, \n    [250, 170, 160],  # other        =   2,\n    [220,  20,  60],  # pedestrian   =   3, \n    [153, 153, 153],  # pole         =   4,\n    [157, 234,  50],  # road line    =   5, \n    [128,  64, 128],  # road         =   6,\n    [244,  35, 232],  # sidewalk     =   7,\n    [107, 142,  35],  # vegetation   =   8, \n    [  0,   0, 142],  # car          =   9,\n    [102, 102, 156],  # wall         =  10, \n    [220, 220,   0],  # traffic sign =  11,\n    [ 60, 250, 240],  # anomaly      =  12,\n]) \n\nclass StreetHazardsClasses(IntEnum):\n    BUILDING        = 0\n    FENCE           = 1\n    OTHER           = 2\n    PEDESTRIAN      = 3\n    POLE            = 4\n    ROAD_LINE       = 5\n    ROAD            = 6\n    SIDEWALK        = 7\n    VEGETATION      = 8\n    CAR             = 9\n    WALL            = 10\n    TRAFFIC_SIGN    = 11\n    ANOMALY         = 12\n    \n#path to streethazards dataset\ntrain_odgt_file = f\"{DATA_DIR}/streethazards_train/train.odgt\"\nval_odgt_file = f\"{DATA_DIR}/streethazards_train/validation.odgt\"\ntest_odgt_file = f\"{DATA_DIR}/streethazards_test/test.odgt\"\n\nCOMPUTE_MEAN_STD = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T22:31:35.108798Z","iopub.execute_input":"2025-10-05T22:31:35.109497Z","iopub.status.idle":"2025-10-05T22:31:35.115425Z","shell.execute_reply.started":"2025-10-05T22:31:35.109471Z","shell.execute_reply":"2025-10-05T22:31:35.114615Z"}},"outputs":[],"execution_count":7},{"id":"2c3c741d-2b47-4a83-ae1c-dd3518c8bca3","cell_type":"code","source":"class StreetHazardsDataset(Dataset):\n    \"\"\"\n    A custom PyTorch Dataset for the StreetHazards inliner dataset.\n\n    This dataset reads image and segmentation label paths from a `.odgt` file,\n    applies optional resizing and spatial transformations, and returns\n    dictionary-style samples with normalized image tensors and label tensors.\n\n    Args:\n        odgt_file (str): Path to the `.odgt` file containing image and label metadata.\n        image_resize (Tuple[int, int], optional): Target size to resize images and labels. \n        spatial_transforms (Callable, optional): Optional transformation function applied to both images and labels.\n        mean_std (Tuple[List[float], List[float]], optional): Mean and standard deviation for image normalization.\n        \n    \"\"\"\n    def __init__(\n        self,\n        odgt_file: str,\n        image_resize: Tuple[int, int] = (512, 896),\n        spatial_transforms: Optional[Callable] = None,\n        mean_std: Tuple[List[float], List[float]] = None\n    ):\n\n        self.spatial_transforms = spatial_transforms\n        self.mean_std = mean_std\n        self.image_resize = image_resize\n\n        with open(odgt_file, \"r\") as f:\n            odgt_data = json.load(f)\n        \n\n        self.paths = [\n            {\n                \"image\": os.path.join(Path(odgt_file).parent, data[\"fpath_img\"]),\n                \"labels\": os.path.join(Path(odgt_file).parent, data[\"fpath_segm\"]),\n            }\n            for data in odgt_data \n        ]\n    \n    def __len__(self) -> int:\n        return len(self.paths)\n\n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n\n        image = Image.open(self.paths[idx][\"image\"]).convert(\"RGB\")\n        labels = Image.open(self.paths[idx][\"labels\"])\n\n        if self.image_resize:\n            image = transforms.Resize(self.image_resize, transforms.InterpolationMode.BILINEAR)(image)\n            labels = transforms.Resize(self.image_resize, transforms.InterpolationMode.NEAREST)(labels)\n            \n        if self.spatial_transforms:\n            image, labels  = self.spatial_transforms(image, labels)         \n\n        #to_tensor\n        image = transforms.ToTensor()(image)\n        labels = torch.as_tensor(transforms.functional.pil_to_tensor(labels), dtype=torch.int64) - 1\n        \n        labels = labels.squeeze(0)\n        \n        if self.mean_std:\n            image = transforms.Normalize(mean = self.mean_std[0], std = self.mean_std[1])(image)\n\n        return {'image' : image, 'labels' : labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T22:58:16.254315Z","iopub.execute_input":"2025-10-05T22:58:16.254960Z","iopub.status.idle":"2025-10-05T22:58:16.262925Z","shell.execute_reply.started":"2025-10-05T22:58:16.254937Z","shell.execute_reply":"2025-10-05T22:58:16.262202Z"}},"outputs":[],"execution_count":37},{"id":"1e639d25-b113-466a-bbd6-1b3f63dba4f5","cell_type":"code","source":"def create_one_hot_prototypes_torch(num_known_classes: int, t_value: float = 3.0, device: str = 'cpu') -> torch.Tensor:\n    \"\"\"\n    Generates one-hot prototypes as a PyTorch tensor for a given number of known classes.\n    Each prototype is a vector where only the element corresponding\n    to its class index has the 't_value', and all other elements are 0.\n\n    Args:\n        num_known_classes (int): The total number of known (in-distribution) classes.\n                                 This also determines the dimensionality of each prototype vector.\n        t_value (float): The non-zero value at the class's specific index in the prototype.\n                         As specified in the paper, this is often 3.0.\n        device (str): The device on which to create the tensor ('cpu' or 'cuda').\n\n    Returns:\n        torch.Tensor: A 2D PyTorch tensor where each row is a prototype vector.\n                      The shape will be (num_known_classes, num_known_classes).\n    \"\"\"\n    if not isinstance(num_known_classes, int) or num_known_classes <= 0:\n        raise ValueError(\"num_known_classes must be a positive integer.\")\n    if not isinstance(t_value, (int, float)):\n        raise ValueError(\"t_value must be a numeric type.\")\n    if device not in ['cpu', 'cuda']:\n        raise ValueError(\"device must be 'cpu' or 'cuda'.\")\n\n    # Create a tensor of zeros\n    prototypes = torch.zeros((num_known_classes, num_known_classes), dtype=torch.float32, device=device)\n\n    # Fill the diagonal with t_value to create one-hot prototypes\n    for i in range(num_known_classes):\n        prototypes[i, i] = t_value\n        \n    # An even more concise way using torch.eye (Identity matrix)\n    # prototypes = torch.eye(num_known_classes, dtype=torch.float32, device=device) * t_value\n\n    return prototypes\n\nPrototype  = create_one_hot_prototypes_torch(12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T22:58:16.759911Z","iopub.execute_input":"2025-10-05T22:58:16.760510Z","iopub.status.idle":"2025-10-05T22:58:16.766451Z","shell.execute_reply.started":"2025-10-05T22:58:16.760485Z","shell.execute_reply":"2025-10-05T22:58:16.765689Z"}},"outputs":[],"execution_count":38},{"id":"1cdbac4a-2fd4-41ad-b843-afe80eee1965","cell_type":"code","source":"\nclass DMLNetFeatureExtractor(torch.nn.Module):\n    def __init__(self, encoder_name, encoder_weights, num_feature_channels, activation):\n        super().__init__()\n        \n\n        self.model = smp.DeepLabV3Plus(\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            classes=num_feature_channels, # This sets the output channels of the segmentation_head if kept\n            activation=activation # Usually 'None' for the main head, but for features it might not matter directly\n        )\n        \n\n        # Option 2 is safer and more robust.\n        # First, we disable the original segmentation head as you did.\n        self.original_segmentation_head = self.model.segmentation_head # Store it if needed\n        self.model.segmentation_head = torch.nn.Identity() # Remove the final head\n\n        # --- CORRECTION START ---\n        # To get the decoder's actual output channels, we need a dummy forward pass\n        # through just the encoder and decoder.\n        \n        # Temporarily detach the module to make a dummy pass if needed,\n        # but in __init__, we can usually just do a conceptual forward.\n        # However, to be absolutely safe and get the runtime channel count:\n        \n        # Create a dummy input to trace the decoder output channels\n        # Assuming typical RGB input (3 channels) and arbitrary spatial dimensions\n        dummy_input = torch.randn(2, 3, 256, 256) \n        \n        # Pass through encoder\n        encoder_features_dummy = self.model.encoder(dummy_input)\n        \n        # Pass through decoder to get its output channels\n        decoder_output_dummy = self.model.decoder(encoder_features_dummy)\n        \n        # Extract the channel dimension from the dummy output\n        decoder_actual_out_channels = decoder_output_dummy.shape[1]\n        # --- CORRECTION END ---\n\n        # Add a 1x1 convolution to project the decoder's output to the desired num_feature_channels.\n        self.feature_projection = torch.nn.Conv2d(\n            in_channels=decoder_actual_out_channels,\n            out_channels=num_feature_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0\n        )\n        \n    def forward(self, x):\n        # The encoder outputs a list of feature maps at different resolutions\n        encoder_features = self.model.encoder(x)\n        \n        # The decoder takes these features and produces a high-resolution feature map.\n        # This output will typically have the same spatial dimensions as the input 'x'\n        # (due to DeepLabV3+ decoder's upsampling) but with its default channel count.\n        decoder_output = self.model.decoder(encoder_features)\n        \n        # Project the decoder's output to the desired number of feature channels\n        final_features = self.feature_projection(decoder_output)\n        \n        # These `final_features` are your f(X; θf)i,j with num_feature_channels.\n        return final_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T23:11:49.410340Z","iopub.execute_input":"2025-10-05T23:11:49.411086Z","iopub.status.idle":"2025-10-05T23:11:49.417896Z","shell.execute_reply.started":"2025-10-05T23:11:49.411061Z","shell.execute_reply":"2025-10-05T23:11:49.417108Z"}},"outputs":[],"execution_count":66},{"id":"66ab273d-1112-4714-ad74-b66bd28d7af1","cell_type":"code","source":"class DiscriminativeCrossEntropyLoss(nn.Module):\n    def __init__(self, prototypes: torch.Tensor, reduction: str = 'mean'):\n        super().__init__()\n       \n        self.prototypes = prototypes\n        self.reduction = reduction\n        if prototypes.dim() != 2:\n            raise ValueError(\"Prototypes must be a 2D tensor (num_classes, feature_dim)\")\n\n    def forward(self, pixel_features: torch.Tensor, target_labels: torch.Tensor):\n        \n        if pixel_features.dim() != 2:\n            raise ValueError(\"pixel_features must be a 2D tensor (N_pixels, feature_dim)\")\n        if target_labels.dim() != 1:\n            raise ValueError(\"target_labels must be a 1D tensor (N_pixels,)\")\n            \n        num_pixels, feature_dim = pixel_features.shape\n        num_classes, proto_feature_dim = self.prototypes.shape\n\n        if feature_dim != proto_feature_dim:\n            raise ValueError(f\"Feature dimension mismatch: pixel_features ({feature_dim}) \"\n                             f\"vs prototypes ({proto_feature_dim})\")\n        \n        # 1. Calculate squared Euclidean distances from each pixel feature to ALL prototypes\n        #   (N_pixels, feature_dim) - (num_classes, feature_dim) -> broadcasting\n        #   Resulting distances_sq: (N_pixels, num_classes)\n        \n        # A bit more efficient way to compute all-pairs squared Euclidean distances:\n        # ||a - b||^2 = ||a||^2 - 2<a,b> + ||b||^2\n        \n        # Calculate ||a||^2 for pixel_features\n        pixel_features_sq_norm = torch.sum(pixel_features**2, dim=1, keepdim=True) # (N_pixels, 1)\n        # Calculate ||b||^2 for prototypes\n        prototypes_sq_norm = torch.sum(self.prototypes**2, dim=1, keepdim=True).T # (1, num_classes)\n        \n        # Calculate 2<a,b>\n        dot_product = torch.matmul(pixel_features, self.prototypes.T) * 2 # (N_pixels, num_classes)\n        \n        # Combine to get squared distances\n        # (N_pixels, 1) + (1, num_classes) - (N_pixels, num_classes)\n        distances_sq = pixel_features_sq_norm + prototypes_sq_norm - dot_product\n        \n        # Ensure distances are non-negative due to potential floating point inaccuracies\n        distances_sq = torch.clamp(distances_sq, min=0.0)\n\n        # 2. Transform squared distances into \"logits\" (or similarity scores)\n        # The paper uses exp(-distance^2)\n        # logits_from_distances will be (N_pixels, num_classes)\n        logits_from_distances = -distances_sq \n        # Note: applying exp() *after* this would be like a custom softmax.\n        # However, nn.CrossEntropyLoss expects raw logits, so we keep them as -distance_sq\n        # If the formula in the paper is a custom \"softmax\", then using F.log_softmax\n        # on -distances_sq directly is the closest match for the structure of cross_entropy.\n\n        # Let's verify the paper's formula with F.log_softmax/NLLLoss:\n        # log( exp(A) / sum(exp(B)) ) = log_softmax(A)\n        # So, the inner part of log is exactly a softmax on -distances_sq\n        \n        # F.log_softmax on -distances_sq\n        log_probabilities = F.log_softmax(logits_from_distances, dim=1) # (N_pixels, num_classes)\n\n        # NLLLoss expects log-probabilities\n        # F.nll_loss directly calculates -log_probabilities[target_labels]\n        # target_labels should be long type and contain class indices (0 to num_classes-1)\n        loss = F.nll_loss(log_probabilities, target_labels, reduction=self.reduction)\n        \n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T23:11:52.622366Z","iopub.execute_input":"2025-10-05T23:11:52.622983Z","iopub.status.idle":"2025-10-05T23:11:52.630202Z","shell.execute_reply.started":"2025-10-05T23:11:52.622959Z","shell.execute_reply":"2025-10-05T23:11:52.629337Z"}},"outputs":[],"execution_count":67},{"id":"72317ee6-d700-429f-a24e-cfa7a1eae00e","cell_type":"code","source":"shape_resize = (512, 896)\n\nif COMPUTE_MEAN_STD:\n    mean_streethazards, std_streethazards = compute_mean_std_channels(StreetHazardsDataset(odgt_file= train_odgt_file,\n                                                                                           image_resize = shape_resize,\n                                                                                           spatial_transforms=None,\n                                                                                           mean_std=None))\nelse:\n    mean_streethazards, std_streethazards = [0.3302, 0.3459, 0.373], [0.1595, 0.1577, 0.1712]\n\nspatial_transforms = transforms.v2.Compose([\n    transforms.v2.RandomHorizontalFlip(),\n])\n\ntrain_dataset = StreetHazardsDataset(\n    odgt_file= train_odgt_file,\n    image_resize = shape_resize,\n    spatial_transforms=spatial_transforms,\n    mean_std=(mean_streethazards, std_streethazards)\n)\n\nval_dataset = StreetHazardsDataset(\n    odgt_file= val_odgt_file,\n    image_resize = shape_resize,\n    spatial_transforms=None,\n    mean_std=(mean_streethazards, std_streethazards)\n)\n\ntest_dataset = StreetHazardsDataset(\n    odgt_file= test_odgt_file,\n    image_resize = shape_resize,\n    spatial_transforms=None,\n    mean_std=(mean_streethazards, std_streethazards)\n)\n\ntrain_dl = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\nval_dl = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\ntest_dl = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T23:11:52.949661Z","iopub.execute_input":"2025-10-05T23:11:52.950216Z","iopub.status.idle":"2025-10-05T23:11:53.033230Z","shell.execute_reply.started":"2025-10-05T23:11:52.950186Z","shell.execute_reply":"2025-10-05T23:11:53.032683Z"}},"outputs":[],"execution_count":68},{"id":"17e83874-2e71-4235-a6ce-edd9448c3d8e","cell_type":"code","source":"torch.cuda.empty_cache()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Esempio di utilizzo:\nencoder_name = \"resnet34\"\nencoder_weights = \"imagenet\"\nnum_known_classes = 12 # Numero di classi per cui i prototipi sono one-hot\nt_value = 3.0\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Inizializza il feature extractor\nfeature_extractor = DMLNetFeatureExtractor(\n    encoder_name=encoder_name,\n    encoder_weights=encoder_weights,\n    num_feature_channels=num_known_classes,\n    activation=None\n).to(device)\nmodel_optimizer = torch.optim.Adam(feature_extractor.parameters(), lr=0.001)\n\ndef train(num_epochs,model,train_loader,verbose= False) -> None:\n        \n        for epoch in tqdm(range(num_epochs), desc=\"Epoch\"):\n            \n            model\n\n            losses = []\n\n            for batch in train_loader: \n                    \n                imgs = batch['image'].to(device)\n                labels = batch['labels'].to(device)\n                \n                logits = model(imgs)\n                print(logits.shape)\n                \n                if type(logits) == tuple:\n                    \n                    vanilla_logits, logits = logits\n                    loss_res = self.loss1(logits=logits, targets=labels.clone())\n                else:\n    \n                    if not self.loss2:\n                        loss_res = self.loss1(logits, labels)\n    \n                    else:\n\n                        loss1_res = self.loss1(logits, labels)\n                        loss2_res = self.loss2(logits, labels)\n                        loss_res = self.loss_scheduler(loss1= loss1_res, loss2= loss2_res, epoch= epoch)\n                        \n                    del imgs, labels\n                            \n                losses.append(loss_res.item())\n                \n                self.optimizer.zero_grad()\n                loss_res.backward()\n                self.optimizer.step()\n                self.scheduler.step()\n            \n                del loss_res\n                \n\n            l = sum(losses) / len(losses)\n\n            print(f\"Epoch {epoch + 1}\", end = ' ')\n            self.eval(\"train\", epoch)\n            self.eval(\"val\", epoch)\n\n            if self.patience and self.patience < self.step:\n                if self.wandb_login:\n                    wandb.finish()\n                break\n\n        if self.wandb_login:\n            wandb.finish()\n\ntrain(10,feature_extractor,train_dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T23:11:53.183681Z","iopub.execute_input":"2025-10-05T23:11:53.184474Z","iopub.status.idle":"2025-10-05T23:11:53.927766Z","shell.execute_reply.started":"2025-10-05T23:11:53.184445Z","shell.execute_reply":"2025-10-05T23:11:53.926803Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_187/1497674044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnum_feature_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_known_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m ).to(device)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmodel_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 3.12 MiB is free. Process 48725 has 15.88 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 47.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 3.12 MiB is free. Process 48725 has 15.88 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 47.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":69},{"id":"e4a4cdc6-7b1f-44dc-bc76-28bea2b44c33","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}